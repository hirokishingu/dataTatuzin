# python 機会学習プログラミング達人データサイエンティスト個人学習用

## CH2
- 教師あり学習の線形分岐の基本概念。
- 勾配降下法のベクトル化実装
- 確率的勾配降下法によるオンライン学習に基づき、 ADALINEを効率よくトレーニング

## CH3
> 機械学習のアルゴリズムのトレーニング５ステップ
- 1, 特徴量を選択する
- 2, 性能指標を選択する
- 3, 分類機と最適化アルゴリズムを選択する。
- 4, モデルの性能を評価する。
- 5, アルゴリズムを調整する.

> Sigmoid関数
- サンプルがクラス１に属する確率を表す.
- 尤度：結果から見られる条件のもっともらしさ.

> サポートベクターマシン(SVM)
- SVMの最適化の目的はマージンを最大かすること。マージンは、超平面（決定境界）とこの超平面にもっとも近いトレーニングサンプルとの間の距離として定義される。
- 超平面に最も近いトレーニングサンプルはサポートベクトルと呼ばれる.
> カーネルSVM
- 「カーネル」は２つのサンプル間の「類似性を表す関数」であると解釈できる.
- マイナス記号をつけているのは、距離の指標を反転させて類似度にするため。
> 決定木学習
- 得られた結果の意味を解釈しやすいかどうかに配慮する場合に魅力的なモデルである。
- 二分決定木で使用される不純度の指標または分割条件は、ジニ不純度・エントロピー・分類誤差の３つである.




















